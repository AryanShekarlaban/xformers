


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>xFormers 0.0.1 documentation</title>
  
  <script src="_static/js/ga.js"></script>
  <script src="_static/js/redirect.js"></script>
  
  <link rel="shortcut icon" href="_static/images/favicon.png" />
  
  
  <link rel="canonical" href="https://fairinternal.github.io/xformersindex.html" />
  
  <meta property="og:title" content="xFormers 0.0.1 documentation">
  <meta name="description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <meta property="og:description" content="API docs for xFormers. xFormers is a PyTorch extension library for composable and optimized Transformer blocks.">
  <!--<meta property="og:image" content="https://mmf.sh/img/logo.png">-->
  <!--<meta property="twitter:image" content="https://mmf.sh/img/logo.png">-->
  <meta name="twitter:image:alt" content="Image for xFormers">
  <meta name="twitter:card" content="summary_large_image">
  

  
  
  

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/css/customize.css" type="text/css" />
  <link rel="index" title="Index" href="genindex.html" />
  <link rel="search" title="Search" href="search.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://github.com/facebookresearch/xformers"><img src="_static/logo.png"
          style="padding-right: 90px;" width="280px" height="53px"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers"> xFormers Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          

          
        </div>

        
        
        
        
        <!-- Local TOC -->
        <div class="local-toc"><p><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-what_is_xformers">What is xFormers?</a></li>
</ul>
<p><span class="caption-text">Components Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-components/index">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-components/attentions">Attention mechanisms</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-components/feedforward">Feedforward mechanisms</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-components/position_embedding">Position Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-components/reversible">Reversible layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-components/mha">Multi Head Attention</a></li>
</ul>
</li>
</ul>
<p><span class="caption-text">Factory</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-factory/index">Factory</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-factory/block">Block factory</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-factory/model">Model factory</a></li>
</ul>
</li>
</ul>
<p><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-tutorials/index">Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-tutorials/sparse_vit">Replace all attentions from an existing ViT model with a sparse equivalent?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-tutorials/blocksparse">Using BlockSparseAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-tutorials/extend_attentions">Extend the xFormers parts zoo</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-tutorials/use_attention">I’m only interested in testing out the attention mechanisms that are hosted here</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-tutorials/pytorch_encoder">I’m used to PyTorch Transformer Encoder, do you have an equivalent?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-tutorials/reversible">Using the Reversible block</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-tutorials/triton">Using Triton-based layers</a></li>
</ul>
</li>
</ul>
<p><span class="caption-text">Custom parts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-custom_parts/index">Custom parts reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#sparse-cuda-kernels">Sparse CUDA kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#triton-parts">Triton parts</a></li>
</ul>
</li>
</ul>
<p><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-tools/index">Tools</a><ul class="simple">
</ul>
</li>
</ul>
</div>
        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="#">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>xFormers 0.0.1 documentation</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">

        
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="welcome-to-xformers-s-documentation">
<h1>Welcome to xFormers’s documentation!<a class="headerlink" href="#welcome-to-xformers-s-documentation" title="Permalink to this headline">¶</a></h1>
<p><em>xFormers</em> is a PyTorch based library which hosts flexible Transformers parts.
They are interoperable and optimized building blocks, which can be optionally be combined
to create some state of the art models.</p>
<div class="toctree-wrapper compound">
<span id="document-what_is_xformers"></span><section id="what-is-xformers">
<h2>What is xFormers?<a class="headerlink" href="#what-is-xformers" title="Permalink to this headline">¶</a></h2>
<p>Flexible Transformers, defined by interoperable and optimized building blocks.</p>
<a class="reference internal image-reference" href="_images/logo.png"><img alt="_images/logo.png" class="align-center" src="_images/logo.png" style="width: 700px; height: 205px;" /></a>
<p>xFormers is focused on the following values</p>
<ul class="simple">
<li><p><strong>Field agnostic</strong>. This library is not focused on any given field, by design.</p></li>
<li><p><strong>Composable</strong>. Ideally, break all the Transformer inspired models into a <em>block zoo</em>, which allows you to compose reference models but also study ablations or architecture search.</p></li>
<li><p><strong>Extensible</strong>. xFormers aims at being <em>easy to extend locally</em>, so that one can focus on a specific improvement, and easily compare it against the state of the art.</p></li>
<li><p><strong>Optimized</strong>. Reusing building blocks across domains means that engineering efforts can be more valued. And since you cannot improve what you cannot measure, xFormers is benchmark-heavy.</p></li>
<li><p><strong>Tested</strong>. Each and every of the variant in the repo is tested, alone and composed with the other relevant blocks. This happens automatically anytime somebody proposes a new variant through a PR.</p></li>
<li><p><strong>Crowd Sourced</strong>. PRs are really welcome, the state of the art is moving too fast for anything but a crowd sourced effort.</p></li>
</ul>
</section>
</div>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
<div class="toctree-wrapper compound">
<span id="document-components/index"></span><section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-components/attentions"></span><section id="module-xformers.components.attention">
<span id="attention-mechanisms"></span><h3>Attention mechanisms<a class="headerlink" href="#module-xformers.components.attention" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="xformers.components.attention.ScaledDotProduct">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.attention.</code><code class="sig-name descname">ScaledDotProduct</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">causal</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">seq_len</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">to_seq_len</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.ScaledDotProduct" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xformers.components.attention.base.Attention</span></code></p>
<p>Implementing the Scaled Dot-Product attention proposed in
<a class="reference external" href="https://arxiv.org/abs/1706.03762v5">Attention is all you need</a>, Vaswani et al.</p>
<dl class="py attribute">
<dt id="xformers.components.attention.ScaledDotProduct.mask">
<code class="sig-name descname">mask</code><em class="property">: Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></em><a class="headerlink" href="#xformers.components.attention.ScaledDotProduct.mask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="xformers.components.attention.ScaledDotProduct.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">q</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">k</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">att_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><a class="headerlink" href="#xformers.components.attention.ScaledDotProduct.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>att_mask    A 2D or 3D mask which ignores attention at certain positions. A value of True will keep the</dt><dd><p>value, while a value of False will mask the value. Key padding masks
(dimension: batch x sequence length) and attention masks
(dimension: sequence length x sequence length OR batch x sequence length x sequence length)
can be combined and passed in here. Method maybe_merge_masks provided in the utils can be
used for that merging. Additive masks are not yet supported.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.components.attention.LocalAttention">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.attention.</code><code class="sig-name descname">LocalAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">causal</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">window_size</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span> <span class="o">=</span> <span class="default_value">5</span></em>, <em class="sig-param"><span class="n">force_sparsity</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.LocalAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xformers.components.attention.base.Attention</span></code></p>
<dl class="py method">
<dt id="xformers.components.attention.LocalAttention.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">causal</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">window_size</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span> <span class="o">=</span> <span class="default_value">5</span></em>, <em class="sig-param"><span class="n">force_sparsity</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.LocalAttention.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>An implementation of a sliding window attention, as proposed in <a class="reference external" href="https://arxiv.org/pdf/2003.05997.pdf">RoutingTransformer</a>, <a class="reference external" href="https://arxiv.org/pdf/2004.05150.pdf">LongFormer</a> or <a class="reference external" href="https://arxiv.org/pdf/2007.14062.pdf">BigBird</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – the probability of an output to be randomly dropped at training time</p></li>
<li><p><strong>causal</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – apply a causal mask, in that the attention cannot be applied to the future</p></li>
<li><p><strong>window_size</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – the overall window size for local attention.
Odd number is expected if the mask is not causal, as the window size will be evenly
distributed on both sides of each query</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="xformers.components.attention.LocalAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">q</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">k</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">att_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.LocalAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.components.attention.LinformerAttention">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.attention.</code><code class="sig-name descname">LinformerAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span></em>, <em class="sig-param"><span class="n">seq_len</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span></em>, <em class="sig-param"><span class="n">k</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.LinformerAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xformers.components.attention.base.Attention</span></code></p>
<dl class="py method">
<dt id="xformers.components.attention.LinformerAttention.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span></em>, <em class="sig-param"><span class="n">seq_len</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span></em>, <em class="sig-param"><span class="n">k</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.LinformerAttention.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Linformer attention mechanism,
from <a class="reference external" href="https://arxiv.org/abs/2006.04768v2">Linformer: Self-Attention with Linear Complexity</a>, Wang et al (2020).
The original notation is kept as is.</p>
</dd></dl>

<dl class="py method">
<dt id="xformers.components.attention.LinformerAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">q</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">k</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.LinformerAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.components.attention.NystromAttention">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.attention.</code><code class="sig-name descname">NystromAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span></em>, <em class="sig-param"><span class="n">num_heads</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span></em>, <em class="sig-param"><span class="n">num_landmarks</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span> <span class="o">=</span> <span class="default_value">64</span></em>, <em class="sig-param"><span class="n">landmark_pooling</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">causal</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">use_razavi_pinverse</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">pinverse_original_init</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">inv_iterations</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span> <span class="o">=</span> <span class="default_value">6</span></em>, <em class="sig-param"><span class="n">v_skip_connection</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">conv_kernel_size</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.NystromAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xformers.components.attention.base.Attention</span></code></p>
<dl class="py method">
<dt id="xformers.components.attention.NystromAttention.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span></em>, <em class="sig-param"><span class="n">num_heads</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span></em>, <em class="sig-param"><span class="n">num_landmarks</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span> <span class="o">=</span> <span class="default_value">64</span></em>, <em class="sig-param"><span class="n">landmark_pooling</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">causal</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">use_razavi_pinverse</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">pinverse_original_init</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">inv_iterations</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span> <span class="o">=</span> <span class="default_value">6</span></em>, <em class="sig-param"><span class="n">v_skip_connection</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">conv_kernel_size</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.NystromAttention.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Nystrom attention mechanism, from <a class="reference external" href="https://arxiv.org/pdf/2102.03902.pdf">Nystromformer</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;A Nystrom-based Algorithm for Approximating Self-Attention.&quot;</span>
<span class="n">Xiong</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="p">,</span> <span class="n">Zeng</span><span class="p">,</span> <span class="n">Z</span><span class="o">.</span><span class="p">,</span> <span class="n">Chakraborty</span><span class="p">,</span> <span class="n">R</span><span class="o">.</span><span class="p">,</span> <span class="n">Tan</span><span class="p">,</span> <span class="n">M</span><span class="o">.</span><span class="p">,</span> <span class="n">Fung</span><span class="p">,</span> <span class="n">G</span><span class="o">.</span><span class="p">,</span> <span class="n">Li</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="p">,</span> <span class="n">Singh</span><span class="p">,</span> <span class="n">V</span><span class="o">.</span> <span class="p">(</span><span class="mi">2021</span><span class="p">)</span>

<span class="n">Reference</span> <span class="n">codebase</span><span class="p">:</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">mlpen</span><span class="o">/</span><span class="n">Nystromformer</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="xformers.components.attention.NystromAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">q</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">k</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">key_padding_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.NystromAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>key_padding_mask    Only a key padding mask is accepted here. The size must be (batch size, sequence length) or</dt><dd><p>(batch size * num_heads, 1, sequence length). If dimensions are not correct, the mask will
be ignored.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.components.attention.RandomAttention">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.attention.</code><code class="sig-name descname">RandomAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span></em>, <em class="sig-param"><span class="n">causal</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">r</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span> <span class="o">=</span> <span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">constant_masking</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">force_sparsity</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.RandomAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xformers.components.attention.base.Attention</span></code></p>
<dl class="py method">
<dt id="xformers.components.attention.RandomAttention.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span></em>, <em class="sig-param"><span class="n">causal</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">r</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span> <span class="o">=</span> <span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">constant_masking</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">force_sparsity</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.RandomAttention.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>“Random” attention, as proposed for instance in _BigBird.
Random means in that case means that each query can attend to a random set of keys.
This implementation is sparse-aware, meaning that the empty attention parts will not be represented in memory.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>r</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – the ratio in [0,1] of keys that the query can attend to</p></li>
<li><p><strong>constant_masking</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – if true, keep the same random set for all queries.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>_BigBird: “Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula,</dt><dd><p>A., Wang, Q., Yang, L., &amp; Ahmed, A. (2020). Big Bird: Transformers for Longer Sequences. ArXiv, NeurIPS.”</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="xformers.components.attention.RandomAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">q</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">k</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">att_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.RandomAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.components.attention.OrthoFormerAttention">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.attention.</code><code class="sig-name descname">OrthoFormerAttention</code><span class="sig-paren">(</span><em class="sig-param">dropout: float</em>, <em class="sig-param">num_landmarks: int = 32</em>, <em class="sig-param">subsample_fraction: float = 1.0</em>, <em class="sig-param">landmark_selection: xformers.components.attention.ortho.LandmarkSelection = &lt;LandmarkSelection.Orthogonal: 'orthogonal'&gt;</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.OrthoFormerAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xformers.components.attention.base.Attention</span></code></p>
<dl class="py method">
<dt id="xformers.components.attention.OrthoFormerAttention.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">dropout: float</em>, <em class="sig-param">num_landmarks: int = 32</em>, <em class="sig-param">subsample_fraction: float = 1.0</em>, <em class="sig-param">landmark_selection: xformers.components.attention.ortho.LandmarkSelection = &lt;LandmarkSelection.Orthogonal: 'orthogonal'&gt;</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.OrthoFormerAttention.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://arxiv.org/abs/2106.05392">Orthoformer</a> attention mechanism.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers&quot;</span>
<span class="n">Patrick</span><span class="p">,</span> <span class="n">M</span><span class="o">.</span><span class="p">,</span> <span class="n">Campbell</span><span class="p">,</span> <span class="n">D</span><span class="o">.</span><span class="p">,</span> <span class="n">Asano</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="p">,</span> <span class="n">Misra</span><span class="p">,</span> <span class="n">I</span><span class="o">.</span><span class="p">,</span> <span class="n">Metze</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="p">,</span> <span class="n">Feichtenhofer</span><span class="p">,</span>
<span class="n">C</span><span class="o">.</span><span class="p">,</span> <span class="n">Vedaldi</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="p">,</span> <span class="n">Henriques</span><span class="p">,</span> <span class="n">J</span><span class="o">.</span> <span class="p">(</span><span class="mi">2021</span><span class="p">)</span>

<span class="n">Reference</span> <span class="n">codebase</span><span class="p">:</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">facebookresearch</span><span class="o">/</span><span class="n">Motionformer</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="xformers.components.attention.OrthoFormerAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">q</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">k</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">att_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.OrthoFormerAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.components.attention.GlobalAttention">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.attention.</code><code class="sig-name descname">GlobalAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span></em>, <em class="sig-param"><span class="n">attention_query_mask</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">causal</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">force_sparsity</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">_</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">__</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.GlobalAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xformers.components.attention.base.Attention</span></code></p>
<dl class="py method">
<dt id="xformers.components.attention.GlobalAttention.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span></em>, <em class="sig-param"><span class="n">attention_query_mask</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">causal</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">force_sparsity</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">_</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">__</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.GlobalAttention.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Global attention, as proposed for instance in <a class="reference external" href="https://arxiv.org/pdf/2007.14062.pdf">BigBird</a> or <a class="reference external" href="https://arxiv.org/pdf/2004.05150.pdf">Longformer</a>.</p>
<p>Global means in that case that the queries positively labelled in the <code class="docutils literal notranslate"><span class="pre">`attention_query_mask`</span></code> can attend
to all the other queries. The queries negatively labelled in the <code class="docutils literal notranslate"><span class="pre">`attention_query_mask`</span></code> cannot attend to
any other query.</p>
<p>This implementation is sparse-aware, meaning that the empty attention parts will not be represented in memory.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – probability of an element to be zeroed</p></li>
<li><p><strong>attention_mask</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))"><em>torch.Tensor</em></a>) – if true, this query can attend to all the others</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="xformers.components.attention.GlobalAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">q</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">k</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">att_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">_</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">__</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.GlobalAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.components.attention.FavorAttention">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.attention.</code><code class="sig-name descname">FavorAttention</code><span class="sig-paren">(</span><em class="sig-param">causal: bool = False</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">dim_features: Optional[int] = None</em>, <em class="sig-param">dim_head: Optional[int] = None</em>, <em class="sig-param">iter_before_redraw: Optional[int] = None</em>, <em class="sig-param">feature_map_type: xformers.components.attention.feature_maps.FeatureMapType = &lt;FeatureMapType.SMReg: 'sm_reg'&gt;</em>, <em class="sig-param">normalize_inputs: bool = False</em>, <em class="sig-param">*_</em>, <em class="sig-param">**__</em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.FavorAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xformers.components.attention.base.Attention</span></code></p>
<dl class="py method">
<dt id="xformers.components.attention.FavorAttention.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">causal: bool = False</em>, <em class="sig-param">dropout: float = 0.0</em>, <em class="sig-param">dim_features: Optional[int] = None</em>, <em class="sig-param">dim_head: Optional[int] = None</em>, <em class="sig-param">iter_before_redraw: Optional[int] = None</em>, <em class="sig-param">feature_map_type: xformers.components.attention.feature_maps.FeatureMapType = &lt;FeatureMapType.SMReg: 'sm_reg'&gt;</em>, <em class="sig-param">normalize_inputs: bool = False</em>, <em class="sig-param">*_</em>, <em class="sig-param">**__</em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.FavorAttention.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Kernelized attention, as proposed in <a class="reference external" href="&quot;Rethinkingattentionwithperformers.&quot;K.Choromanskietal.(2020).https://arxiv.org/pdf/2009.14794v1.pdf">Performers</a>.</p>
<p>FAVOR stands for “Fast Attention Via positive Orthogonal Random features”</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – the probability of an output to be randomly dropped at training time</p></li>
<li><p><strong>dim_features</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – the dimension of the random features space</p></li>
<li><p><strong>iter_before_redraw</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – the number of iterations before a redraw of the features</p></li>
<li><p><strong>feature_map_type</strong> (<em>FeatureMapType</em>) – the type of feature map being used,</p></li>
<li><p><strong>instance orthogonal random features.</strong> (<em>for</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="xformers.components.attention.FavorAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">q</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">k</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.FavorAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.components.attention.Attention">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.attention.</code><code class="sig-name descname">Attention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.Attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The base Attention mechanism, which is typically a sub-part of the multi-head attention</p>
<dl class="py method">
<dt id="xformers.components.attention.Attention.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">xformers.components.attention.base.AttentionConfig</span></em><span class="sig-paren">)</span> &#x2192; Self<a class="headerlink" href="#xformers.components.attention.Attention.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="xformers.components.attention.Attention.forward">
<em class="property">abstract </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">q</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">k</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><a class="headerlink" href="#xformers.components.attention.Attention.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="xformers.components.attention.build_attention">
<code class="sig-prename descclassname">xformers.components.attention.</code><code class="sig-name descname">build_attention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span><span class="p">, </span>xformers.components.attention.base.AttentionConfig<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.build_attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Builds an attention from a config.</p>
<p>This assumes a ‘name’ key in the config which is used to determine what
attention class to instantiate. For instance, a config <cite>{“name”: “my_attention”,
“foo”: “bar”}</cite> will find a class that was registered as “my_attention”
(see <a class="reference internal" href="#xformers.components.attention.register_attention" title="xformers.components.attention.register_attention"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_attention()</span></code></a>) and call .from_config on it.</p>
</dd></dl>

<dl class="py function">
<dt id="xformers.components.attention.register_attention">
<code class="sig-prename descclassname">xformers.components.attention.</code><code class="sig-name descname">register_attention</code><span class="sig-paren">(</span><em class="sig-param">name: str</em>, <em class="sig-param">config: Any = &lt;class 'xformers.components.attention.base.AttentionConfig'&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.attention.register_attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a subclass.</p>
<p>This decorator allows xFormers to instantiate a given subclass
from a configuration file, even if the class itself is not part of the
xFormers library.</p>
</dd></dl>

</section>
<span id="document-components/feedforward"></span><section id="module-xformers.components.feedforward">
<span id="feedforward-mechanisms"></span><h3>Feedforward mechanisms<a class="headerlink" href="#module-xformers.components.feedforward" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="xformers.components.feedforward.MLP">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.feedforward.</code><code class="sig-name descname">MLP</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dim_model</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span></em>, <em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span></em>, <em class="sig-param"><span class="n">activation</span><span class="p">:</span> <span class="n">xformers.components.activations.Activation</span></em>, <em class="sig-param"><span class="n">hidden_layer_multiplier</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.feedforward.MLP" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xformers.components.feedforward.base.Feedforward</span></code></p>
<dl class="py method">
<dt id="xformers.components.feedforward.MLP.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><a class="headerlink" href="#xformers.components.feedforward.MLP.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.components.feedforward.MLP.training">
<code class="sig-name descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></em><a class="headerlink" href="#xformers.components.feedforward.MLP.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.components.feedforward.Feedforward">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.feedforward.</code><code class="sig-name descname">Feedforward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dim_model</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">activation</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>xformers.components.activations.Activation<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.feedforward.Feedforward" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt id="xformers.components.feedforward.Feedforward.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">xformers.components.feedforward.base.FeedforwardConfig</span></em><span class="sig-paren">)</span> &#x2192; Self<a class="headerlink" href="#xformers.components.feedforward.Feedforward.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.components.feedforward.Feedforward.training">
<code class="sig-name descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></em><a class="headerlink" href="#xformers.components.feedforward.Feedforward.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="xformers.components.feedforward.build_feedforward">
<code class="sig-prename descclassname">xformers.components.feedforward.</code><code class="sig-name descname">build_feedforward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span><span class="p">, </span>xformers.components.feedforward.base.FeedforwardConfig<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.feedforward.build_feedforward" title="Permalink to this definition">¶</a></dt>
<dd><p>Builds a feedforward from a config.</p>
<p>This assumes a ‘name’ key in the config which is used to determine what
attention class to instantiate. For instance, a config <cite>{“name”: “my_feedforward”,
“foo”: “bar”}</cite> will find a class that was registered as “my_feedforward”
(see <a class="reference internal" href="#xformers.components.feedforward.register_feedforward" title="xformers.components.feedforward.register_feedforward"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_feedforward()</span></code></a>) and call .from_config on it.</p>
</dd></dl>

<dl class="py function">
<dt id="xformers.components.feedforward.register_feedforward">
<code class="sig-prename descclassname">xformers.components.feedforward.</code><code class="sig-name descname">register_feedforward</code><span class="sig-paren">(</span><em class="sig-param">name: str</em>, <em class="sig-param">config: Any = &lt;class 'xformers.components.feedforward.base.FeedforwardConfig'&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.feedforward.register_feedforward" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a subclass.</p>
<p>This decorator allows xFormers to instantiate a given subclass
from a configuration file, even if the class itself is not part of the
xFormers library.</p>
</dd></dl>

</section>
<span id="document-components/position_embedding"></span><section id="module-xformers.components.positional_embedding">
<span id="position-embeddings"></span><h3>Position Embeddings<a class="headerlink" href="#module-xformers.components.positional_embedding" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="xformers.components.positional_embedding.SinePositionalEmbedding">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.positional_embedding.</code><code class="sig-name descname">SinePositionalEmbedding</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dim_model</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.positional_embedding.SinePositionalEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xformers.components.positional_embedding.base.PositionEmbedding</span></code></p>
<dl class="py method">
<dt id="xformers.components.positional_embedding.SinePositionalEmbedding.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.positional_embedding.SinePositionalEmbedding.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.components.positional_embedding.SinePositionalEmbedding.training">
<code class="sig-name descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></em><a class="headerlink" href="#xformers.components.positional_embedding.SinePositionalEmbedding.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.components.positional_embedding.VocabEmbedding">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.positional_embedding.</code><code class="sig-name descname">VocabEmbedding</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dim_model</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span></em>, <em class="sig-param"><span class="n">seq_len</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span></em>, <em class="sig-param"><span class="n">vocab_size</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span></em>, <em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.positional_embedding.VocabEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">xformers.components.positional_embedding.base.PositionEmbedding</span></code></p>
<dl class="py method">
<dt id="xformers.components.positional_embedding.VocabEmbedding.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.positional_embedding.VocabEmbedding.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.components.positional_embedding.VocabEmbedding.training">
<code class="sig-name descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></em><a class="headerlink" href="#xformers.components.positional_embedding.VocabEmbedding.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="xformers.components.positional_embedding.build_positional_embedding">
<code class="sig-prename descclassname">xformers.components.positional_embedding.</code><code class="sig-name descname">build_positional_embedding</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span><span class="p">, </span>xformers.components.positional_embedding.base.PositionEmbeddingConfig<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.positional_embedding.build_positional_embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Builds a position encoding from a config.</p>
<p>This assumes a ‘name’ key in the config which is used to determine what
attention class to instantiate. For instance, a config <cite>{“name”: “my_position_encoding”,
“foo”: “bar”}</cite> will find a class that was registered as “my_position_encoding”
(see <a class="reference internal" href="#xformers.components.positional_embedding.register_positional_embedding" title="xformers.components.positional_embedding.register_positional_embedding"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_positional_embedding()</span></code></a>) and call .from_config on it.</p>
</dd></dl>

<dl class="py function">
<dt id="xformers.components.positional_embedding.register_positional_embedding">
<code class="sig-prename descclassname">xformers.components.positional_embedding.</code><code class="sig-name descname">register_positional_embedding</code><span class="sig-paren">(</span><em class="sig-param">name: str</em>, <em class="sig-param">config: Any = &lt;class 'xformers.components.positional_embedding.base.PositionEmbeddingConfig'&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.positional_embedding.register_positional_embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a subclass.</p>
<p>This decorator allows xFormers to instantiate a given subclass
from a configuration file, even if the class itself is not part of the
xFormers library.</p>
</dd></dl>

</section>
<span id="document-components/reversible"></span><section id="reversible-layer">
<h3>Reversible layer<a class="headerlink" href="#reversible-layer" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="xformers.components.reversible.Deterministic">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.reversible.</code><code class="sig-name descname">Deterministic</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">net</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.reversible.Deterministic" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt id="xformers.components.reversible.Deterministic.record_rng">
<code class="sig-name descname">record_rng</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.reversible.Deterministic.record_rng" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="xformers.components.reversible.Deterministic.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="n">record_rng</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">set_rng</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.reversible.Deterministic.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.components.reversible.Deterministic.training">
<code class="sig-name descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></em><a class="headerlink" href="#xformers.components.reversible.Deterministic.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.components.reversible.ReversibleBlock">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.reversible.</code><code class="sig-name descname">ReversibleBlock</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">f</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">g</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.reversible.ReversibleBlock" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt id="xformers.components.reversible.ReversibleBlock.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">f_args</span><span class="o">=</span><span class="default_value">{}</span></em>, <em class="sig-param"><span class="n">g_args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.reversible.ReversibleBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="xformers.components.reversible.ReversibleBlock.backward_pass">
<code class="sig-name descname">backward_pass</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">y</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">dy</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">f_args</span><span class="o">=</span><span class="default_value">{}</span></em>, <em class="sig-param"><span class="n">g_args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.reversible.ReversibleBlock.backward_pass" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.components.reversible.ReversibleBlock.training">
<code class="sig-name descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></em><a class="headerlink" href="#xformers.components.reversible.ReversibleBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.components.reversible.ReversibleSequence">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.reversible.</code><code class="sig-name descname">ReversibleSequence</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">blocks</span><span class="p">:</span> <span class="n">torch.nn.modules.container.ModuleList</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.reversible.ReversibleSequence" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt id="xformers.components.reversible.ReversibleSequence.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">arg_route</span><span class="o">=</span><span class="default_value">True, False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.reversible.ReversibleSequence.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.components.reversible.ReversibleSequence.training">
<code class="sig-name descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></em><a class="headerlink" href="#xformers.components.reversible.ReversibleSequence.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<span id="document-components/mha"></span><section id="multi-head-attention">
<h3>Multi Head Attention<a class="headerlink" href="#multi-head-attention" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="xformers.components.MultiHeadDispatch">
<em class="property">class </em><code class="sig-prename descclassname">xformers.components.</code><code class="sig-name descname">MultiHeadDispatch</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dim_model</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span></em>, <em class="sig-param"><span class="n">residual_dropout</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)">float</a></span></em>, <em class="sig-param"><span class="n">num_heads</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span></em>, <em class="sig-param"><span class="n">attention</span><span class="p">:</span> <span class="n">xformers.components.attention.base.Attention</span></em>, <em class="sig-param"><span class="n">bias</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">dim_key</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dim_value</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">in_proj_container</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>xformers.components.in_proj_container.InProjContainer<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_separate_proj_weight</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">out_proj</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.MultiHeadDispatch" title="Permalink to this definition">¶</a></dt>
<dd><p>A multi-head masked self-attention dispatch mechanism, with a projection at the end,
following the architecture proposed in <a class="reference external" href="https://arxiv.org/abs/1706.03762v5">Attention is all you need</a>, Vaswani et al.</p>
<p>The actual attention mechanism can vary, as well as the projections.
This can be used to wrap the proposed attention mechanisms and make them multi-head aware,
but it is optional.</p>
<dl class="py method">
<dt id="xformers.components.MultiHeadDispatch.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">query</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">key</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">value</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">att_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><a class="headerlink" href="#xformers.components.MultiHeadDispatch.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Expected input dimensions are [batch size, sequence length, embed dim]
Output dimensions are [batch size, sequence length, embed dim]</p>
</dd></dl>

<dl class="py method">
<dt id="xformers.components.MultiHeadDispatch.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">xformers.components.multi_head_dispatch.MultiHeadDispatchConfig</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.components.MultiHeadDispatch.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.components.MultiHeadDispatch.training">
<code class="sig-name descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></em><a class="headerlink" href="#xformers.components.MultiHeadDispatch.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</div>
</section>
</div>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
<div class="toctree-wrapper compound">
<span id="document-factory/index"></span><section id="factory">
<h2>Factory<a class="headerlink" href="#factory" title="Permalink to this headline">¶</a></h2>
<p>Factories are completely optional, they were primarily developed for CI and benchmarking purposes.</p>
<div class="toctree-wrapper compound">
<span id="document-factory/block"></span><section id="module-xformers.factory.block_factory">
<span id="block-factory"></span><h3>Block factory<a class="headerlink" href="#module-xformers.factory.block_factory" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="xformers.factory.block_factory.LayerPositionBitmask">
<em class="property">class </em><code class="sig-prename descclassname">xformers.factory.block_factory.</code><code class="sig-name descname">LayerPositionBitmask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.LayerPositionBitmask" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>, <a class="reference external" href="https://docs.python.org/3.6/library/enum.html#enum.Enum" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></a></p>
<p>An enumeration.</p>
<dl class="py attribute">
<dt id="xformers.factory.block_factory.LayerPositionBitmask.First">
<code class="sig-name descname">First</code><em class="property"> = 1</em><a class="headerlink" href="#xformers.factory.block_factory.LayerPositionBitmask.First" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.block_factory.LayerPositionBitmask.Last">
<code class="sig-name descname">Last</code><em class="property"> = 2</em><a class="headerlink" href="#xformers.factory.block_factory.LayerPositionBitmask.Last" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.block_factory.LayerPositionBitmask.Default">
<code class="sig-name descname">Default</code><em class="property"> = 3</em><a class="headerlink" href="#xformers.factory.block_factory.LayerPositionBitmask.Default" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.factory.block_factory.LayerPosition">
<em class="property">class </em><code class="sig-prename descclassname">xformers.factory.block_factory.</code><code class="sig-name descname">LayerPosition</code><a class="headerlink" href="#xformers.factory.block_factory.LayerPosition" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Bitmask to mark this layer as first, last, nothing or both</p>
<dl class="py method">
<dt id="xformers.factory.block_factory.LayerPosition.is_first">
<code class="sig-name descname">is_first</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.LayerPosition.is_first" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="xformers.factory.block_factory.LayerPosition.is_last">
<code class="sig-name descname">is_last</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.LayerPosition.is_last" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="xformers.factory.block_factory.LayerPosition.mark_not_first">
<code class="sig-name descname">mark_not_first</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.LayerPosition.mark_not_first" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="xformers.factory.block_factory.LayerPosition.mark_not_last">
<code class="sig-name descname">mark_not_last</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.LayerPosition.mark_not_last" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.factory.block_factory.BlockType">
<em class="property">class </em><code class="sig-prename descclassname">xformers.factory.block_factory.</code><code class="sig-name descname">BlockType</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.BlockType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference external" href="https://docs.python.org/3.6/library/enum.html#enum.Enum" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></a></p>
<p>An enumeration.</p>
<dl class="py attribute">
<dt id="xformers.factory.block_factory.BlockType.Encoder">
<code class="sig-name descname">Encoder</code><em class="property"> = 'encoder'</em><a class="headerlink" href="#xformers.factory.block_factory.BlockType.Encoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.block_factory.BlockType.Decoder">
<code class="sig-name descname">Decoder</code><em class="property"> = 'decoder'</em><a class="headerlink" href="#xformers.factory.block_factory.BlockType.Decoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.factory.block_factory.xFormerBlockConfig">
<em class="property">class </em><code class="sig-prename descclassname">xformers.factory.block_factory.</code><code class="sig-name descname">xFormerBlockConfig</code><span class="sig-paren">(</span><em class="sig-param">dim_model: int, feedforward_config: Dict[str, Any], position_encoding_config: Union[Dict[str, Any], NoneType], block_type: xformers.factory.block_factory.BlockType, layer_norm_style: xformers.components.residual.LayerNormStyle = &lt;LayerNormStyle.Post: 'post'&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.xFormerBlockConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<dl class="py attribute">
<dt id="xformers.factory.block_factory.xFormerBlockConfig.use_triton">
<code class="sig-name descname">use_triton</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></em><a class="headerlink" href="#xformers.factory.block_factory.xFormerBlockConfig.use_triton" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.block_factory.xFormerBlockConfig.dim_model">
<code class="sig-name descname">dim_model</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></em><a class="headerlink" href="#xformers.factory.block_factory.xFormerBlockConfig.dim_model" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.block_factory.xFormerBlockConfig.block_type">
<code class="sig-name descname">block_type</code><em class="property">: <a class="reference internal" href="index.html#xformers.factory.block_factory.BlockType" title="xformers.factory.block_factory.BlockType">xformers.factory.block_factory.BlockType</a></em><a class="headerlink" href="#xformers.factory.block_factory.xFormerBlockConfig.block_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.block_factory.xFormerBlockConfig.layer_norm_style">
<code class="sig-name descname">layer_norm_style</code><em class="property">: xformers.components.residual.LayerNormStyle</em><a class="headerlink" href="#xformers.factory.block_factory.xFormerBlockConfig.layer_norm_style" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.block_factory.xFormerBlockConfig.feedforward_config">
<code class="sig-name descname">feedforward_config</code><em class="property">: xformers.components.feedforward.base.FeedforwardConfig</em><a class="headerlink" href="#xformers.factory.block_factory.xFormerBlockConfig.feedforward_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.block_factory.xFormerBlockConfig.position_encoding_config">
<code class="sig-name descname">position_encoding_config</code><em class="property">: Optional<span class="p">[</span>xformers.components.positional_embedding.base.PositionEmbeddingConfig<span class="p">]</span></em><a class="headerlink" href="#xformers.factory.block_factory.xFormerBlockConfig.position_encoding_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.block_factory.xFormerBlockConfig.layer_position">
<code class="sig-name descname">layer_position</code><em class="property">: <a class="reference internal" href="index.html#xformers.factory.block_factory.LayerPosition" title="xformers.factory.block_factory.LayerPosition">xformers.factory.block_factory.LayerPosition</a></em><a class="headerlink" href="#xformers.factory.block_factory.xFormerBlockConfig.layer_position" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.factory.block_factory.xFormerEncoderConfig">
<em class="property">class </em><code class="sig-prename descclassname">xformers.factory.block_factory.</code><code class="sig-name descname">xFormerEncoderConfig</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dim_model</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span></em>, <em class="sig-param"><span class="n">feedforward_config</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">multi_head_config</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">position_encoding_config</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span><span class="p">, </span>NoneType<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">layer_norm_style</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a></span> <span class="o">=</span> <span class="default_value">'post'</span></em>, <em class="sig-param"><span class="n">use_triton</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">_</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.xFormerEncoderConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#xformers.factory.block_factory.xFormerBlockConfig" title="xformers.factory.block_factory.xFormerBlockConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">xformers.factory.block_factory.xFormerBlockConfig</span></code></a></p>
<dl class="py attribute">
<dt id="xformers.factory.block_factory.xFormerEncoderConfig.multi_head_config">
<code class="sig-name descname">multi_head_config</code><em class="property">: Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span></em><a class="headerlink" href="#xformers.factory.block_factory.xFormerEncoderConfig.multi_head_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.block_factory.xFormerEncoderConfig.use_triton">
<code class="sig-name descname">use_triton</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></em><a class="headerlink" href="#xformers.factory.block_factory.xFormerEncoderConfig.use_triton" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.factory.block_factory.xFormerDecoderConfig">
<em class="property">class </em><code class="sig-prename descclassname">xformers.factory.block_factory.</code><code class="sig-name descname">xFormerDecoderConfig</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dim_model</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></span></em>, <em class="sig-param"><span class="n">feedforward_config</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">multi_head_config_masked</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">multi_head_config_cross</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">position_encoding_config</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span><span class="p">, </span>NoneType<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">layer_norm_style</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a></span> <span class="o">=</span> <span class="default_value">'post'</span></em>, <em class="sig-param"><span class="n">use_triton</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">_</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.xFormerDecoderConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#xformers.factory.block_factory.xFormerBlockConfig" title="xformers.factory.block_factory.xFormerBlockConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">xformers.factory.block_factory.xFormerBlockConfig</span></code></a></p>
<dl class="py attribute">
<dt id="xformers.factory.block_factory.xFormerDecoderConfig.multi_head_config_masked">
<code class="sig-name descname">multi_head_config_masked</code><em class="property">: Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span></em><a class="headerlink" href="#xformers.factory.block_factory.xFormerDecoderConfig.multi_head_config_masked" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.block_factory.xFormerDecoderConfig.multi_head_config_cross">
<code class="sig-name descname">multi_head_config_cross</code><em class="property">: Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span></em><a class="headerlink" href="#xformers.factory.block_factory.xFormerDecoderConfig.multi_head_config_cross" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.factory.block_factory.xFormerEncoderBlock">
<em class="property">class </em><code class="sig-prename descclassname">xformers.factory.block_factory.</code><code class="sig-name descname">xFormerEncoderBlock</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n"><a class="reference internal" href="index.html#xformers.factory.block_factory.xFormerEncoderConfig" title="xformers.factory.block_factory.xFormerEncoderConfig">xformers.factory.block_factory.xFormerEncoderConfig</a></span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.xFormerEncoderBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>A vanilla Transformer Encoder block</p>
<dl class="py method">
<dt id="xformers.factory.block_factory.xFormerEncoderBlock.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n"><a class="reference internal" href="index.html#xformers.factory.block_factory.xFormerEncoderConfig" title="xformers.factory.block_factory.xFormerEncoderConfig">xformers.factory.block_factory.xFormerEncoderConfig</a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.xFormerEncoderBlock.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="xformers.factory.block_factory.xFormerEncoderBlock.get_reversible_layer">
<em class="property">static </em><code class="sig-name descname">get_reversible_layer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>torch.nn.modules.module.Module<span class="p">, </span>torch.nn.modules.module.Module<span class="p">]</span><a class="headerlink" href="#xformers.factory.block_factory.xFormerEncoderBlock.get_reversible_layer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="xformers.factory.block_factory.xFormerEncoderBlock.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">att_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">input_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.xFormerEncoderBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.block_factory.xFormerEncoderBlock.training">
<code class="sig-name descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></em><a class="headerlink" href="#xformers.factory.block_factory.xFormerEncoderBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.factory.block_factory.xFormerDecoderBlock">
<em class="property">class </em><code class="sig-prename descclassname">xformers.factory.block_factory.</code><code class="sig-name descname">xFormerDecoderBlock</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n"><a class="reference internal" href="index.html#xformers.factory.block_factory.xFormerDecoderConfig" title="xformers.factory.block_factory.xFormerDecoderConfig">xformers.factory.block_factory.xFormerDecoderConfig</a></span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.xFormerDecoderBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>A vanilla Transformer Decoder block</p>
<p>… note: this implementation is not (yet ?) reversible</p>
<dl class="py attribute">
<dt id="xformers.factory.block_factory.xFormerDecoderBlock.training">
<code class="sig-name descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></em><a class="headerlink" href="#xformers.factory.block_factory.xFormerDecoderBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="xformers.factory.block_factory.xFormerDecoderBlock.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n"><a class="reference internal" href="index.html#xformers.factory.block_factory.xFormerDecoderConfig" title="xformers.factory.block_factory.xFormerDecoderConfig">xformers.factory.block_factory.xFormerDecoderConfig</a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.xFormerDecoderBlock.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="xformers.factory.block_factory.xFormerDecoderBlock.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">target</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">memory</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">encoder_att_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">decoder_att_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">input_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.block_factory.xFormerDecoderBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<span id="document-factory/model"></span><section id="module-xformers.factory.model_factory">
<span id="model-factory"></span><h3>Model factory<a class="headerlink" href="#module-xformers.factory.model_factory" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="xformers.factory.model_factory.xFormerStackConfig">
<em class="property">class </em><code class="sig-prename descclassname">xformers.factory.model_factory.</code><code class="sig-name descname">xFormerStackConfig</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">block_config</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">reversible</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.model_factory.xFormerStackConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>A stack is defined by the definition of a given block, and an optional repetition factor</p>
<dl class="py attribute">
<dt id="xformers.factory.model_factory.xFormerStackConfig.block_config">
<code class="sig-name descname">block_config</code><em class="property">: Union<span class="p">[</span><a class="reference internal" href="index.html#xformers.factory.block_factory.xFormerEncoderConfig" title="xformers.factory.block_factory.xFormerEncoderConfig">xformers.factory.block_factory.xFormerEncoderConfig</a><span class="p">, </span><a class="reference internal" href="index.html#xformers.factory.block_factory.xFormerDecoderConfig" title="xformers.factory.block_factory.xFormerDecoderConfig">xformers.factory.block_factory.xFormerDecoderConfig</a><span class="p">]</span></em><a class="headerlink" href="#xformers.factory.model_factory.xFormerStackConfig.block_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.model_factory.xFormerStackConfig.num_layers">
<code class="sig-name descname">num_layers</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)">int</a></em><a class="headerlink" href="#xformers.factory.model_factory.xFormerStackConfig.num_layers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.model_factory.xFormerStackConfig.reversible">
<code class="sig-name descname">reversible</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></em><a class="headerlink" href="#xformers.factory.model_factory.xFormerStackConfig.reversible" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.factory.model_factory.xFormerConfig">
<em class="property">class </em><code class="sig-prename descclassname">xformers.factory.model_factory.</code><code class="sig-name descname">xFormerConfig</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">stack_configs</span><span class="p">:</span> <span class="n">List<span class="p">[</span>Dict<span class="p">[</span><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)">str</a><span class="p">, </span>Any<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.model_factory.xFormerConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<dl class="py attribute">
<dt id="xformers.factory.model_factory.xFormerConfig.stack_configs">
<code class="sig-name descname">stack_configs</code><em class="property">: List<span class="p">[</span><a class="reference internal" href="index.html#xformers.factory.model_factory.xFormerStackConfig" title="xformers.factory.model_factory.xFormerStackConfig">xformers.factory.model_factory.xFormerStackConfig</a><span class="p">]</span></em><a class="headerlink" href="#xformers.factory.model_factory.xFormerConfig.stack_configs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="xformers.factory.model_factory.xFormer">
<em class="property">class </em><code class="sig-prename descclassname">xformers.factory.model_factory.</code><code class="sig-name descname">xFormer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">stack_configs</span><span class="p">:</span> <span class="n">List<span class="p">[</span><a class="reference internal" href="index.html#xformers.factory.model_factory.xFormerStackConfig" title="xformers.factory.model_factory.xFormerStackConfig">xformers.factory.model_factory.xFormerStackConfig</a><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.model_factory.xFormer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="py method">
<dt id="xformers.factory.model_factory.xFormer.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">stack_configs</span><span class="p">:</span> <span class="n">List<span class="p">[</span><a class="reference internal" href="index.html#xformers.factory.model_factory.xFormerStackConfig" title="xformers.factory.model_factory.xFormerStackConfig">xformers.factory.model_factory.xFormerStackConfig</a><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.model_factory.xFormer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a serialized configuration, generate the corresponding model.
This is only a helper and can easily be bypassed</p>
</dd></dl>

<dl class="py attribute">
<dt id="xformers.factory.model_factory.xFormer.training">
<code class="sig-name descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)">bool</a></em><a class="headerlink" href="#xformers.factory.model_factory.xFormer.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="xformers.factory.model_factory.xFormer.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n"><a class="reference internal" href="index.html#xformers.factory.model_factory.xFormerConfig" title="xformers.factory.model_factory.xFormerConfig">xformers.factory.model_factory.xFormerConfig</a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#xformers.factory.model_factory.xFormer.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="xformers.factory.model_factory.xFormer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">src</span><span class="p">:</span> <span class="n"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a></span></em>, <em class="sig-param"><span class="n">tgt</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">encoder_input_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">decoder_input_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; Optional<span class="p">[</span><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.11.0a0+git52fec21 ))">torch.Tensor</a><span class="p">]</span><a class="headerlink" href="#xformers.factory.model_factory.xFormer.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</div>
</section>
</div>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
<div class="toctree-wrapper compound">
<span id="document-tutorials/index"></span><section id="tutorials">
<h2>Tutorials<a class="headerlink" href="#tutorials" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-tutorials/sparse_vit"></span><section id="replace-all-attentions-from-an-existing-vit-model-with-a-sparse-equivalent">
<h3>Replace all attentions from an existing ViT model with a sparse equivalent?<a class="headerlink" href="#replace-all-attentions-from-an-existing-vit-model-with-a-sparse-equivalent" title="Permalink to this headline">¶</a></h3>
<p>Let’s say you’re used to working with a given Transformer based model, and want to experiment with one of the attention mechanisms supported by xFormers.</p>
<p>The following example shows how to do that in a particular example (reusing a reference ViT from <a class="reference external" href="https://github.com/rwightman/pytorch-image-models">pytorch-image-models</a>), but some aspects will translate just as well
when considering other model sources. In any case, please check the notebooks in the <a class="reference external" href="https://github.com/facebookresearch/xformers">repository</a> for a more exhaustive take.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">timm</span>
<span class="kn">from</span> <span class="nn">timm.models.vision_transformer</span> <span class="kn">import</span> <span class="n">VisionTransformer</span>
<span class="kn">from</span> <span class="nn">xformers.components.attention</span> <span class="kn">import</span> <span class="n">ScaledDotProduct</span>
<span class="kn">from</span> <span class="nn">xformers.components.attention.helpers</span> <span class="kn">import</span> <span class="n">TimmAttentionWrapper</span>
<span class="n">img_size</span> <span class="o">=</span> <span class="mi">224</span>
<span class="n">patch_size</span> <span class="o">=</span> <span class="mi">16</span>

<span class="c1"># Get a reference ViT model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">VisionTransformer</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
                            <span class="n">embed_dim</span><span class="o">=</span><span class="mi">96</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">3.</span><span class="p">,</span>
                            <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">norm_layer</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>


<span class="c1"># Define the mask that we want to use</span>
<span class="c1"># We suppose in this snipper that you have a precise mask in mind already</span>
<span class="c1"># but several helpers and examples are proposed in  `xformers.components.attention.attention_patterns`</span>
<span class="n">my_fancy_mask</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>  <span class="c1"># This would be for you to define</span>

<span class="c1"># Define a recursive monkey patching function</span>
<span class="k">def</span> <span class="nf">replace_attn_with_xformers_one</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">att_mask</span><span class="p">):</span>
    <span class="n">module_output</span> <span class="o">=</span> <span class="n">module</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">timm</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">vision_transformer</span><span class="o">.</span><span class="n">Attention</span><span class="p">):</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">qkv</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">module</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="c1"># Extra parameters can be exposed in TimmAttentionWrapper, this is a minimal example</span>
        <span class="n">module_output</span> <span class="o">=</span> <span class="n">TimmAttentionWrapper</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">module</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">att_mask</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="n">module_output</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">replace_attn_with_xformers_one</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">att_mask</span><span class="p">))</span>
    <span class="k">del</span> <span class="n">module</span>
    <span class="k">return</span> <span class="n">module_output</span>

<span class="c1"># Now we can just patch our reference model, and get a sparse-aware variation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">replace_attn_with_xformers_one</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that in practice exchanging all the attentions with a sparse alternative may not be a good idea, as the attentions closer to the output are not typically exhibiting a clear sparsity pattern. You can alter <cite>replace_attn_with_xformers_one</cite> above, or replace manually the attentions which would like to sparsify, but not all</p>
</section>
<span id="document-tutorials/blocksparse"></span><section id="using-blocksparseattention">
<h3>Using BlockSparseAttention<a class="headerlink" href="#using-blocksparseattention" title="Permalink to this headline">¶</a></h3>
<p>BlockSparse attention uses <a class="reference external" href="https://github.com/openai/triton">Triton</a> to limit the attention computations to some tiles, which you define at construction time.
A simple example is that of a causal attention: just compute the lower triangular tiles! The tile size can be changed, the minimum being 16 coefficients on one dimension.</p>
<p>If you already have a per-coefficient pattern in mind and this is not a perfect match with a block pattern, this is probably fine,
BlockSparse is fast enough so that dropping some of the computations after the fact with a fine-grained mask is still probably better than dense computations.
We provide a small helper (this is just maxpooling) to convert in between a per coefficient binary mask and the layout that you will need to build a block sparse attention.</p>
<p><em>Please note that for now blocksparse attention requires the sequence length to be a power of two</em>.</p>
<p>Let’s look at an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">xformers.components</span> <span class="kn">import</span> <span class="n">MultiHeadDispatch</span>
<span class="kn">from</span> <span class="nn">xformers.components.attention</span> <span class="kn">import</span> <span class="n">BlockSparseAttention</span>

<span class="n">BATCH</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">HEADS</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">SEQ</span> <span class="o">=</span> <span class="mi">2048</span>
<span class="n">EMB</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">BLOCK_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">DROPOUT</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>

<span class="c1"># Let&#39;s try out a causal mask, but really it could be anything &quot;block sparse enough&quot;</span>
<span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">SEQ</span><span class="p">,</span> <span class="n">SEQ</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>

<span class="n">blocks</span> <span class="o">=</span> <span class="n">SEQ</span> <span class="o">//</span> <span class="n">BLOCK_SIZE</span>
<span class="n">causal_layout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">HEADS</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">blocks</span><span class="p">]))</span>

<span class="c1"># Let&#39;s build our blocksparse attention. Please note that the layout can be</span>
<span class="c1"># [SEQ//BLOCK_SIZE, SEQ//BLOCK_SIZE] or  [HEADS, SEQ//BLOCK_SIZE, SEQ//BLOCK_SIZE]</span>
<span class="c1"># so that _you can pass a different layout per head_</span>
<span class="n">attention</span> <span class="o">=</span> <span class="n">BlockSparseAttention</span><span class="p">(</span><span class="n">layout</span><span class="o">=</span><span class="n">causal_layout</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">DROPOUT</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">HEADS</span><span class="p">)</span>

<span class="c1"># Out of commodity, let&#39;s build our multihead attention now</span>
<span class="c1"># &quot;multi_head&quot; will be responsible for the forward</span>
<span class="n">multi_head</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">MultiHeadDispatch</span><span class="p">(</span>
        <span class="n">seq_len</span><span class="o">=</span><span class="n">SEQ</span><span class="p">,</span>
        <span class="n">dim_model</span><span class="o">=</span><span class="n">EMB</span><span class="p">,</span>
        <span class="n">residual_dropout</span><span class="o">=</span><span class="n">DROPOUT</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">HEADS</span><span class="p">,</span>
        <span class="n">attention</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="p">)</span>

<span class="c1"># Now FW some random data</span>
<span class="c1"># Note that passing a per-coefficient mask makes it possible to remove extra coefficients,</span>
<span class="c1"># which where required by the blockification</span>
<span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">SEQ</span><span class="p">,</span> <span class="n">EMB</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Self attention in this particular example, no limitations really</span>
<span class="n">att_val</span> <span class="o">=</span> <span class="n">multi_head</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">att_mask</span><span class="o">=</span><span class="n">causal_mask</span><span class="p">)</span>


<span class="c1">#########################################</span>
<span class="c1"># Bonus: compare the memory use vs dense:</span>
<span class="k">def</span> <span class="nf">mem_use</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
    <span class="c1"># bookeeping</span>
    <span class="kn">import</span> <span class="nn">time</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>

    <span class="c1"># actually run the function</span>
    <span class="n">fn</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">stop</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="c1"># now report</span>
    <span class="n">max_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="mi">20</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="s2"> - Peak memory use: </span><span class="si">{</span><span class="n">max_memory</span><span class="si">}</span><span class="s2">MB - </span><span class="si">{</span><span class="nb">round</span><span class="p">((</span><span class="n">stop</span><span class="o">-</span><span class="n">start</span><span class="p">)</span><span class="o">*</span><span class="mf">1e6</span><span class="p">)</span><span class="o">/</span><span class="mf">1e3</span><span class="si">}</span><span class="s2">ms&quot;</span><span class="p">)</span>


<span class="n">pytorch_multihead</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span>
    <span class="n">EMB</span><span class="p">,</span> <span class="n">HEADS</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>

<span class="n">mem_use</span><span class="p">(</span><span class="n">multi_head</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">,</span> <span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">,</span> <span class="s2">&quot;att_mask&quot;</span><span class="p">:</span> <span class="n">causal_mask</span><span class="p">},</span> <span class="s2">&quot;Blocksparse&quot;</span><span class="p">)</span>
<span class="n">mem_use</span><span class="p">(</span><span class="n">pytorch_multihead</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">,</span> <span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">,</span> <span class="s2">&quot;attn_mask&quot;</span><span class="p">:</span> <span class="n">causal_mask</span><span class="p">},</span> <span class="s2">&quot;PyTorch&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>On a V100, with PyTorch 1.9, Triton 1.1 and xFormers 0.0.1 this reports something along the lines of:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Blocksparse - Peak memory use: 151MB - <span class="m">6</span>.619ms
PyTorch - Peak memory use: 393MB - <span class="m">6</span>.837ms
</pre></div>
</div>
<p>Note that the pattern here is not that sparse (half of the matrix is empty), the more sparse it gets the more biased the result will get towards BlockSparseAttention.</p>
</section>
<span id="document-tutorials/extend_attentions"></span><section id="extend-the-xformers-parts-zoo">
<h3>Extend the xFormers parts zoo<a class="headerlink" href="#extend-the-xformers-parts-zoo" title="Permalink to this headline">¶</a></h3>
<p>This can be done in a private fork of xFormers, if this is a work in progress or not something that you would
like to share at this point, or directly in xFormers in order to submit a <a class="reference external" href="https://github.com/fairinternal/xformers/pulls">PR</a>.</p>
<p>We follow a register-based architecture, which is practical for unit testing, and loose inheritance
(not all blocks expose the exact same interface).</p>
<p>Let’s consider for instance the Nystrom-based attention mechanism:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">NystromSelfAttentionConfig</span><span class="p">(</span><span class="n">AttentionConfig</span><span class="p">):</span>
    <span class="o">...</span>

<span class="nd">@register_attention</span><span class="p">(</span><span class="s2">&quot;nystrom&quot;</span><span class="p">,</span> <span class="n">NystromSelfAttentionConfig</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">NystromAttention</span><span class="p">(</span><span class="n">Attention</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">landmark_pooling</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_razavi_pinverse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">pinverse_original_init</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">inv_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>  <span class="c1"># recommended default in paper was 6.</span>
        <span class="n">v_skip_connection</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">conv_kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="o">...</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>There are a couple of things to remark, which would be true of any other extension. This also applies to the other components in xformers:</p>
<ul class="simple">
<li><p>The attention mechanism inherits from the base component</p></li>
<li><p>We define a configuration for this block, which is not explicitly used in the constructor, but is required if you want to register this block. It is for instance used for unit testing and benchmarking.</p></li>
<li><p>The construction needs to accept extra <cite>*args, **kwargs</cite> arguments, so that the same configuration can be used by different blocks, even if not all fields are useful</p></li>
<li><p>The registration is done with the following snippet, which both registers this attention with a given name, and ties a configuration to it. The same would apply to the other component types.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@register_attention</span><span class="p">(</span><span class="s2">&quot;nystrom&quot;</span><span class="p">,</span> <span class="n">NystromSelfAttentionConfig</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">NystromAttention</span><span class="p">(</span><span class="n">Attention</span><span class="p">):</span>
</pre></div>
</div>
<p>Doing this opens up at least three tools in the xFormers toolbox:</p>
<ul>
<li><p>the relevant unit tests will now automatically pick up this new variant. You can call all of them in one go with</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pytest</span> <span class="o">-</span><span class="n">x</span> <span class="o">-</span><span class="n">k</span> <span class="n">my_component_name</span>
</pre></div>
</div>
</li>
<li><p>if applicable (attention mechanism), the attention benchmark will pick up this new variant automatically</p></li>
<li><p>the LRA benchmarks will pick up this new block option. You can define a JSON config with your new part and trigger LRA jobs.</p></li>
</ul>
<p>As a reminder (more information in the dedicated README) you can trigger a LRA job locally with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="n">run_tasks</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">attention</span> <span class="o">&lt;</span><span class="n">your</span> <span class="n">attention</span> <span class="n">name</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">task</span> <span class="o">&lt;</span><span class="n">task</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">config</span> <span class="o">&lt;</span><span class="n">config_path</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">world_size</span> <span class="n">N</span>
</pre></div>
</div>
<p>or even submit a batch of jobs to a SLURM enabled cluster with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="n">batch_submit</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">c</span> <span class="n">code</span><span class="o">/</span><span class="n">config</span><span class="o">.</span><span class="n">json</span> <span class="o">-</span><span class="n">ck</span> <span class="o">&lt;</span><span class="n">your</span> <span class="n">checkpoing</span> <span class="ow">and</span> <span class="n">log</span> <span class="n">path</span><span class="o">&gt;</span> <span class="o">-</span><span class="n">a</span> <span class="o">&lt;</span><span class="n">your</span> <span class="n">attention</span> <span class="n">name</span><span class="o">&gt;</span>
</pre></div>
</div>
</section>
<span id="document-tutorials/use_attention"></span><section id="i-m-only-interested-in-testing-out-the-attention-mechanisms-that-are-hosted-here">
<h3>I’m only interested in testing out the attention mechanisms that are hosted here<a class="headerlink" href="#i-m-only-interested-in-testing-out-the-attention-mechanisms-that-are-hosted-here" title="Permalink to this headline">¶</a></h3>
<p>That’s completely fine! There are two paths to do this:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Either you import the attention mechanisms that you’re interested in directly</dt><dd><p>in your code base, their API should be very similar and you would own everything.
The dimensions expectations are that, depending on whether the attentions expose the <cite>requires_head_dimension</cite> flag,
the input data would be either <cite>[Batch, Heads, Sequence, Head dimension]</cite>, or <cite>[Batch x Heads, Sequence, Head dimension]</cite>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Alternatively, a <cite>build_attention</cite> helper is provided, which takes a dict as an input.</dt><dd><p>In that case, you defer a lot of the instantiation work to xFormers,
which makes it a little more obscure although the parameters are hopefully straightforward.
This was initially built for internal use in xFormers, to make sure that we can programatically
build and test all possible combinations.
In turn this should allow you to do sweeps or architecture search, given that the multihead attention definition
becomes something like:</p>
</dd>
</dl>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">xformers.components</span> <span class="kn">import</span> <span class="n">MultiHeadDispatch</span><span class="p">,</span> <span class="n">build_attention</span>
<span class="n">SEQ</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">MODEL</span> <span class="o">=</span> <span class="mi">384</span>
<span class="n">HEADS</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">DROPOUT</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">my_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">attention_name</span><span class="p">,</span>  <span class="c1"># you can easily make this dependent on a file, sweep,..</span>
    <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">DROPOUT</span><span class="p">,</span>
    <span class="s2">&quot;seq_len&quot;</span><span class="p">:</span> <span class="n">SEQ</span><span class="p">,</span>
    <span class="s2">&quot;attention_query_mask&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">SEQ</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">0.3</span><span class="p">,</span> <span class="c1"># some dummy mask</span>
<span class="p">}</span>

<span class="n">attention</span> <span class="o">=</span> <span class="n">build_attention</span><span class="p">(</span><span class="n">my_config</span><span class="p">)</span>

<span class="c1"># build a multi head dispatch to test this attention mechanism</span>
<span class="n">multi_head</span> <span class="o">=</span> <span class="n">MultiHeadDispatch</span><span class="p">(</span>
    <span class="n">seq_len</span><span class="o">=</span><span class="n">SEQ</span><span class="p">,</span>
    <span class="n">dim_model</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
    <span class="n">residual_dropout</span><span class="o">=</span><span class="n">DROPOUT</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="n">HEADS</span><span class="p">,</span>
    <span class="n">attention</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># do something with my new multi-head attention</span>
<span class="c1">#...</span>
</pre></div>
</div>
</section>
<span id="document-tutorials/pytorch_encoder"></span><section id="i-m-used-to-pytorch-transformer-encoder-do-you-have-an-equivalent">
<h3>I’m used to PyTorch Transformer Encoder, do you have an equivalent?<a class="headerlink" href="#i-m-used-to-pytorch-transformer-encoder-do-you-have-an-equivalent" title="Permalink to this headline">¶</a></h3>
<p>PyTorch already exposes a couple of pure Transformer blocks,
for instance TransformerEncoder and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html?highlight=encoder#torch.nn.TransformerEncoderLayer">TransformerEncoderLayer</a>.</p>
<p>Their interfaces are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">TransformerEncoderLayer</span><span class="p">(</span>
    <span class="n">d_model</span><span class="p">,</span>
    <span class="n">nhead</span><span class="p">,</span>
    <span class="n">dim_feedforward</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
    <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span>
    <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
    <span class="o">...</span>

<span class="n">Transformer</span><span class="p">(</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">nhead</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">num_encoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">num_decoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">dim_feedforward</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
    <span class="n">custom_encoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">custom_decoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span>
    <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="o">.</span>
</pre></div>
</div>
<p>While xFormers doesn’t have the exact same interfaces, it has something fairly close
through the <cite>model_factory</cite>.</p>
<p>The equivalent with xFormers would look like the following.
You can think of it as a declaration of the sequence of blocks that you would like instantiated.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">xformers.factory.model_factory</span> <span class="kn">import</span> <span class="n">xFormer</span><span class="p">,</span> <span class="n">xFormerConfig</span>

<span class="n">my_config</span> <span class="o">=</span>  <span class="p">[</span>
    <span class="c1"># A list of the encoder or decoder blocks which constitute the Transformer.</span>
    <span class="c1"># Note that a sequence of different encoder blocks can be used, same for decoders</span>
    <span class="p">{</span>
        <span class="s2">&quot;reversible&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># Optionally make these layers reversible, to save memory</span>
        <span class="s2">&quot;block_config&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;block_type&quot;</span><span class="p">:</span> <span class="s2">&quot;encoder&quot;</span><span class="p">,</span>
            <span class="s2">&quot;num_layers&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>  <span class="c1"># Optional, this means that this config will repeat N times</span>
            <span class="s2">&quot;dim_model&quot;</span><span class="p">:</span> <span class="mi">384</span><span class="p">,</span>
            <span class="s2">&quot;layer_norm_style&quot;</span><span class="p">:</span> <span class="s2">&quot;pre&quot;</span><span class="p">,</span>  <span class="c1"># Optional, pre/post</span>
            <span class="s2">&quot;position_encoding_config&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;vocab&quot;</span><span class="p">,</span>  <span class="c1"># whatever position encodinhg makes sense</span>
                <span class="s2">&quot;dim_model&quot;</span><span class="p">:</span> <span class="mi">384</span><span class="p">,</span>
                <span class="s2">&quot;seq_len&quot;</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span>
                <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="s2">&quot;multi_head_config&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;num_heads&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
                <span class="s2">&quot;dim_model&quot;</span><span class="p">:</span> <span class="mi">384</span><span class="p">,</span>
                <span class="s2">&quot;residual_dropout&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                <span class="s2">&quot;attention&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;linformer&quot;</span><span class="p">,</span> <span class="c1"># whatever attention mechanism</span>
                    <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                    <span class="s2">&quot;causal&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="s2">&quot;seq_len&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">},</span>
            <span class="s2">&quot;feedforward_config&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;MLP&quot;</span><span class="p">,</span>
                <span class="s2">&quot;dim_model&quot;</span><span class="p">:</span> <span class="mi">384</span><span class="p">,</span>
                <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                <span class="s2">&quot;activation&quot;</span><span class="p">:</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
                <span class="s2">&quot;hidden_layer_multiplier&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">xFormerConfig</span><span class="p">(</span><span class="n">my_config</span><span class="p">)</span>  <span class="c1"># This part of xFormers is entirely type checked and needs a config object, could be changed in the future</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">xFormer</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that this exposes a couple more knobs than the PyTorch Transformer interface,
but in turn is probably a little more flexible.
There are a couple of repeated settings here (dimensions mostly),
this is taken care of in the LRA benchmarking config.</p>
</section>
<span id="document-tutorials/reversible"></span><section id="using-the-reversible-block">
<h3>Using the Reversible block<a class="headerlink" href="#using-the-reversible-block" title="Permalink to this headline">¶</a></h3>
<section id="intro">
<h4>Intro<a class="headerlink" href="#intro" title="Permalink to this headline">¶</a></h4>
<p>This block applies to residual paths, and was first proposed by Gomez et al (<a class="footnote-reference brackets" href="#id6" id="id1">1</a>).
Its application in the Transformer (<a class="footnote-reference brackets" href="#id8" id="id2">3</a>) context was first proposed in the <cite>Reformer</cite> (<a class="footnote-reference brackets" href="#id7" id="id3">2</a>) paper,
and is largely unrelated to the other proposals from this paper (LSH and chunked MLP processing).</p>
<p>We use and very lightly adapt the implementation by Robin <a class="reference external" href="https://github.com/RobinBruegger/RevTorch/blob/master/revtorch/revtorch.py">Bruegger</a> and some blocks from <a class="reference external" href="https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reversible.py">LucidRains</a>.</p>
<p>A reversible layer requires two inputs (x1, x2) and produces two outputs (y1, y2)
via two functions F and G, following the relations</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y1</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">F</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">G</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span>
</pre></div>
</div>
<p>In turn, this means that (x1, x2) can be recovered from (y1, y2) (see <a class="footnote-reference brackets" href="#id6" id="id4">1</a> for details)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x2</span> <span class="o">=</span> <span class="n">y2</span> <span class="o">-</span> <span class="n">G</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span>  <span class="c1"># Note that another FW-like pass is needed</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">y1</span> <span class="o">-</span> <span class="n">F</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
</pre></div>
</div>
<p>The effect is comparable to activation checkpointing, in that it opens up for a tradeoff in between GPU memory
and compute. One benefit is that no extra wrap is needed, all the residual paths can be naturally checkpointed.
In a distributed setting, freeing up GPU memory can help using less GPUs, and the saved communication cost can more than make up for the extra compute.</p>
<p>Moreover, if your model is made of a stack of reversible blocks, then the memory requirement does not increase with the number of blocks.</p>
</section>
<section id="transformer">
<h4>Transformer<a class="headerlink" href="#transformer" title="Permalink to this headline">¶</a></h4>
<p>Considering the multi-head attention and feedforward blocks (including the residual paths), one can set F as MHA (+ layer norm) and G as Feedforward (+ layer norm) and get to something very close (but not exactly the same) to the original Transformer formulation from [Vaswani et al.][3], as follows</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y1</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">MHA</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">Feedforward</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span>
</pre></div>
</div>
<p>A difference is that the residual path in the Feedforward deals with the original input, and not the MHA output,
but in practice if <cite>dim(x1) == dim(x2) == dim(model)</cite>, the accuracy should not be affected, as verified in <a class="footnote-reference brackets" href="#id7" id="id5">2</a> and in xFormers.</p>
</section>
<section id="in-practice">
<h4>In practice<a class="headerlink" href="#in-practice" title="Permalink to this headline">¶</a></h4>
<p>This repository exposes two main helpers in <cite>xformers.components.reversible</cite>: ReversibleBlock and ReversibleSequence. <cite>ReversibleBlock</cite> will take <cite>f</cite> and <cite>g</cite> as defined above, and <cite>ReversibleSequence</cite> can combine them sequentially, similarly to <cite>torch.nn.ModuleList</cite>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ReversibleBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">f_args</span><span class="o">=</span><span class="p">{},</span> <span class="n">g_args</span><span class="o">=</span><span class="p">{}):</span>
        <span class="o">...</span>


<span class="k">class</span> <span class="nc">ReversibleSequence</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blocks</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">arg_route</span><span class="o">=</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        arg_route: whether to route the kwargs to f and g</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>Reversible layers are also exposed as a boolean option in when building complete xFormers (which is optional), as defined in <cite>xformers.factory.model_factory</cite>. Please note that the reversible layer is not yet compatible with the use of multiple forward passes and DDP.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">xFormerStackConfig</span><span class="p">:</span>
    <span class="n">block_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">xFormerEncoderConfig</span><span class="p">,</span> <span class="n">xFormerDecoderConfig</span><span class="p">]</span>
    <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">reversible</span><span class="p">:</span> <span class="nb">bool</span>  <span class="c1"># the sequence of layers becomes reversible</span>
</pre></div>
</div>
<dl class="footnote brackets">
<dt class="label" id="id6"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Gomez, A. N., Ren, M., Urtasun, R., &amp; Grosse, R. B. (2017).
The reversible residual network: Backpropagation without storing activations.</p>
</dd>
<dt class="label" id="id7"><span class="brackets">2</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id5">2</a>)</span></dt>
<dd><p>Kitaev, N., Kaiser, Ł., &amp; Levskaya, A. (2020).
Reformer: The Efficient Transformer.</p>
</dd>
<dt class="label" id="id8"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>Vaswani et al.,
Attention is all you need, 2017</p>
</dd>
</dl>
</section>
</section>
<span id="document-tutorials/triton"></span><section id="using-triton-based-layers">
<h3>Using Triton-based layers<a class="headerlink" href="#using-triton-based-layers" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://triton-lang.org/">Triton</a> is a language and compiler for parallel programming, currently applicable to CUDA-enabled GPUs.
It is compatible with PyTorch CUDA Tensors, and can be interfaced directly with pure python code.</p>
<p>PyTorch provides many primitives capable of tranforming tensors, which correspond to operators in each of the supported backends.
There are limits to how many of them can be supported at any point in time, short of supporting a JIT toolchain,
so some operations typical of the Transformer family are supported in PyTorch as a sequence of base operators.</p>
<p>Triton makes it possible to consolidate some of them into ad-hoc fused operators, which are compiled just-in-time.
xFormers proposes a couple of optimized layers, and the goal is to increase their number over time.</p>
<section id="fused-softmax-layer">
<h4>Fused softmax layer<a class="headerlink" href="#fused-softmax-layer" title="Permalink to this headline">¶</a></h4>
<p>This is a drop-in replacement to <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html">torch.nn.softmax</a>, the only limitation being that the softmax operation is limited to the last dimension.
Log-softmax is also available. The actual Triton kernel is very similar to <cite>this tutorial&lt;https://triton-lang.org/getting-started/tutorials/02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py&gt;</cite></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">xformers.triton</span> <span class="kn">import</span> <span class="n">softmax</span><span class="p">,</span> <span class="n">log_softmax</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># Torch AMP, autograd aware</span>
</pre></div>
</div>
<p>The expected throughput, when compared to PyTorch and on a nVidia V100, is along these lines</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 19%" />
<col style="width: 12%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 12%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>torch.float16</p></th>
<th class="head"><p>Unit: GB/s</p></th>
<th class="head"></th>
<th class="head"></th>
<th class="head"></th>
<th class="head"></th>
<th class="head"></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td></td>
<td><p>B=8, M=384, K=128</p></td>
<td><p>B=8, M=784, K=512</p></td>
<td><p>B=4, M=2048, K=384</p></td>
<td><p>B=4, M=3136, K=1024</p></td>
<td><p>B=2, M=1024, K=2048</p></td>
<td><p>B=2, M=2048, K=4096</p></td>
<td><p>B=2, M=4096, K=4096</p></td>
</tr>
<tr class="row-odd"><td><p>pytorch - fw</p></td>
<td><p>170.7</p></td>
<td><p>501.8</p></td>
<td><p>512.0</p></td>
<td><p>597.3</p></td>
<td><p>399.6</p></td>
<td><p>524.3</p></td>
<td><p>553.0</p></td>
</tr>
<tr class="row-even"><td><p>triton  - fw</p></td>
<td><p>153.6</p></td>
<td><p>522.7</p></td>
<td><p>512.0</p></td>
<td><p>716.8</p></td>
<td><p>606.8</p></td>
<td><p>736.4</p></td>
<td><p>775.6</p></td>
</tr>
<tr class="row-odd"><td><p>pytorch - log - fw</p></td>
<td><p>192.0</p></td>
<td><p>545.4</p></td>
<td><p>534.3</p></td>
<td><p>669.0</p></td>
<td><p>496.5</p></td>
<td><p>601.2</p></td>
<td><p>615.4</p></td>
</tr>
<tr class="row-even"><td><p>triton  - log - fw</p></td>
<td><p>153.6</p></td>
<td><p>570.2</p></td>
<td><p>558.5</p></td>
<td><p>748.9</p></td>
<td><p>682.7</p></td>
<td><p>780.2</p></td>
<td><p>799.2</p></td>
</tr>
<tr class="row-odd"><td><p>pytorch - fw+bw</p></td>
<td><p>71.4</p></td>
<td><p>170.7</p></td>
<td><p>168.3</p></td>
<td><p>205.6</p></td>
<td><p>164.7</p></td>
<td><p>196.5</p></td>
<td><p>203.5</p></td>
</tr>
<tr class="row-even"><td><p>triton  - fw+bw</p></td>
<td><p>69.8</p></td>
<td><p>218.2</p></td>
<td><p>211.9</p></td>
<td><p>264.8</p></td>
<td><p>224.4</p></td>
<td><p>271.4</p></td>
<td><p>284.3</p></td>
</tr>
<tr class="row-odd"><td><p>pytorch - log - fw+bw</p></td>
<td><p>78.8</p></td>
<td><p>207.3</p></td>
<td><p>204.8</p></td>
<td><p>255.3</p></td>
<td><p>206.1</p></td>
<td><p>247.3</p></td>
<td><p>255.5</p></td>
</tr>
<tr class="row-even"><td><p>triton  - log - fw+bw</p></td>
<td><p>71.4</p></td>
<td><p>220.1</p></td>
<td><p>213.7</p></td>
<td><p>266.9</p></td>
<td><p>229.1</p></td>
<td><p>273.6</p></td>
<td><p>285.6</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 19%" />
<col style="width: 12%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 12%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>torch.float32</p></th>
<th class="head"><p>Unit: GB/s</p></th>
<th class="head"></th>
<th class="head"></th>
<th class="head"></th>
<th class="head"></th>
<th class="head"></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td></td>
<td><p>B=8, M=384, K=128</p></td>
<td><p>B=8, M=784, K=512</p></td>
<td><p>B=4, M=2048, K=384</p></td>
<td><p>B=4, M=3136, K=1024</p></td>
<td><p>B=2, M=1024, K=2048</p></td>
<td><p>B=2, M=2048, K=4096</p></td>
<td><p>B=2, M=4096, K=4096</p></td>
</tr>
<tr class="row-odd"><td><p>pytorch - fw</p></td>
<td><p>341.3</p></td>
<td><p>660.2</p></td>
<td><p>682.7</p></td>
<td><p>760.2</p></td>
<td><p>555.4</p></td>
<td><p>636.3</p></td>
<td><p>650.5</p></td>
</tr>
<tr class="row-even"><td><p>triton  - fw</p></td>
<td><p>307.2</p></td>
<td><p>678.1</p></td>
<td><p>682.7</p></td>
<td><p>784.0</p></td>
<td><p>712.3</p></td>
<td><p>789.6</p></td>
<td><p>809.1</p></td>
</tr>
<tr class="row-odd"><td><p>pytorch - log - fw</p></td>
<td><p>384.0</p></td>
<td><p>696.9</p></td>
<td><p>702.2</p></td>
<td><p>777.9</p></td>
<td><p>537.2</p></td>
<td><p>541.6</p></td>
<td><p>543.9</p></td>
</tr>
<tr class="row-even"><td><p>triton  - log - fw</p></td>
<td><p>307.2</p></td>
<td><p>696.9</p></td>
<td><p>702.2</p></td>
<td><p>796.4</p></td>
<td><p>744.7</p></td>
<td><p>799.2</p></td>
<td><p>814.1</p></td>
</tr>
<tr class="row-odd"><td><p>pytorch - fw+bw</p></td>
<td><p>133.6</p></td>
<td><p>203.1</p></td>
<td><p>204.0</p></td>
<td><p>229.9</p></td>
<td><p>193.9</p></td>
<td><p>211.1</p></td>
<td><p>215.3</p></td>
</tr>
<tr class="row-even"><td><p>triton  - fw+bw</p></td>
<td><p>136.5</p></td>
<td><p>254.7</p></td>
<td><p>257.3</p></td>
<td><p>290.9</p></td>
<td><p>263.2</p></td>
<td><p>294.5</p></td>
<td><p>301.0</p></td>
</tr>
<tr class="row-odd"><td><p>pytorch - log - fw+bw</p></td>
<td><p>149.9</p></td>
<td><p>252.1</p></td>
<td><p>252.1</p></td>
<td><p>289.6</p></td>
<td><p>234.1</p></td>
<td><p>251.6</p></td>
<td><p>254.5</p></td>
</tr>
<tr class="row-even"><td><p>triton  - log - fw+bw</p></td>
<td><p>136.5</p></td>
<td><p>257.3</p></td>
<td><p>258.7</p></td>
<td><p>291.7</p></td>
<td><p>265.3</p></td>
<td><p>295.2</p></td>
<td><p>301.3</p></td>
</tr>
</tbody>
</table>
</section>
<section id="fused-linear-layer">
<h4>Fused linear layer<a class="headerlink" href="#fused-linear-layer" title="Permalink to this headline">¶</a></h4>
<p>This is a drop-in replacement to two PyTorch operands: a <cite>torch.nn.Linear</cite>, and an activation, like <cite>torch.nn.ReLU</cite>. It is Torch AMP and autograd aware, and can be used very simply:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">xformers.triton</span> <span class="kn">import</span> <span class="n">FusedLinearLayer</span>

<span class="n">my_linear_layer</span> <span class="o">=</span> <span class="n">FusedLinearLayer</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="o">/</span><span class="kc">False</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;squared_relu&quot;</span><span class="p">)</span>

<span class="o">...</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">my_linear_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>It is possible to skip either the bias or the activation (just use <cite>None</cite> in that case). As of September 2021, this layer is <strong>faster than PyTorch for non-sigmoid activations and fp16</strong>.
In all other usecases, you will be better served using PyTorch.</p>
<p>The following is an example of the measured performance on a nVidia V100.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 28%" />
<col style="width: 15%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 15%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>torch.float16</p></th>
<th class="head"><p>Unit: TFlops</p></th>
<th class="head"></th>
<th class="head"></th>
<th class="head"></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td></td>
<td><p>B=8, M=256, K=512</p></td>
<td><p>B=8, M=512, K=1024</p></td>
<td><p>B=4, M=1024, K=1024</p></td>
<td><p>B=2, M=2048, K=2048</p></td>
<td><p>B=2, M=4096, K=4096</p></td>
</tr>
<tr class="row-odd"><td><p>pytorch - squared_relu -  bias - fw</p></td>
<td><p>6.3</p></td>
<td><p>12.4</p></td>
<td><p>12.3</p></td>
<td><p>17.1</p></td>
<td><p>19.0</p></td>
</tr>
<tr class="row-even"><td><p>triton  - squared_relu -  bias - fw</p></td>
<td><p>13.8</p></td>
<td><p>18.9</p></td>
<td><p>18.9</p></td>
<td><p>21.9</p></td>
<td><p>21.7</p></td>
</tr>
<tr class="row-odd"><td><p>pytorch - squared_relu -  bias - fw+bw</p></td>
<td><p>4.0</p></td>
<td><p>7.6</p></td>
<td><p>7.7</p></td>
<td><p>10.7</p></td>
<td><p>12.6</p></td>
</tr>
<tr class="row-even"><td><p>triton  - squared_relu -  bias - fw+bw</p></td>
<td><p>8.4</p></td>
<td><p>13.5</p></td>
<td><p>13.3</p></td>
<td><p>15.9</p></td>
<td><p>16.8</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</div>
</section>
</div>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
<div class="toctree-wrapper compound">
<span id="document-custom_parts/index"></span><section id="custom-parts-reference">
<h2>Custom parts reference<a class="headerlink" href="#custom-parts-reference" title="Permalink to this headline">¶</a></h2>
<section id="sparse-cuda-kernels">
<h3>Sparse CUDA kernels<a class="headerlink" href="#sparse-cuda-kernels" title="Permalink to this headline">¶</a></h3>
<section id="building-the-kernels">
<h4>1. Building the kernels<a class="headerlink" href="#building-the-kernels" title="Permalink to this headline">¶</a></h4>
<p>xFormers transparently supports CUDA kernels to implement sparse attention computations, some of which are based on <a class="reference external" href="https://github.com/google-research/sputnik">Sputnik</a>.
These kernels require xFormers to be installed from source, and the recipient machine to be able to compile CUDA source code.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone git@github.com:fairinternal/xformers.git
conda create --name xformer_env <span class="nv">python</span><span class="o">=</span><span class="m">3</span>.8
conda activate xformer_env
<span class="nb">cd</span> xformers
pip install -r requirements.txt
pip install -e .
</pre></div>
</div>
<p>Common issues are related to:</p>
<ul class="simple">
<li><p>NVCC and the current CUDA runtime match. You can often change the CUDA runtime with <cite>module unload cuda module load cuda/xx.x</cite>, possibly also <cite>nvcc</cite></p></li>
<li><p>the version of GCC that you’re using matches the current NVCC capabilities</p></li>
<li><p>the <cite>TORCH_CUDA_ARCH_LIST</cite> env variable is set to the architures that you want to support. A suggested setup (slow to build but comprehensive) is <cite>export TORCH_CUDA_ARCH_LIST=”6.0;6.1;6.2;7.0;7.2;8.0;8.6”</cite></p></li>
</ul>
</section>
<section id="usage">
<h4>2. Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h4>
<p>The sparse attention computation is automatically triggered when using the <strong>scaled dot product</strong> attention (<a class="reference external" href="https://github.com/facebookresearch/xformers/blob/main/xformers/components/attention/scaled_dot_product.py">see</a>), and a sparse enough mask (currently less than 30% of true values).
There is nothing specific to do, and a couple of examples are provided in the tutorials.</p>
</section>
</section>
<section id="triton-parts">
<h3>Triton parts<a class="headerlink" href="#triton-parts" title="Permalink to this headline">¶</a></h3>
<section id="requirements">
<h4>1. Requirements<a class="headerlink" href="#requirements" title="Permalink to this headline">¶</a></h4>
<p>We use <a class="reference external" href="https://triton-lang.org/">Triton</a> to implement the following parts.
These parts will only be visible on a CUDA-enabled machine, and Triton needs to be installed (<cite>pip install triton</cite>),
if any of these conditions are not met a warning is issued.</p>
</section>
<section id="possible-usage">
<h4>2. Possible usage<a class="headerlink" href="#possible-usage" title="Permalink to this headline">¶</a></h4>
<p>The following parts are independent and can be used as-is in any model,
provided the above limitations (Triton is installed, and there is a CUDA GPU present) are fullfilled.
They are used by default, when possible, in some of the xFormers building blocks.</p>
<span class="target" id="module-xformers.triton"></span></section>
</section>
</section>
</div>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
<div class="toctree-wrapper compound">
<span id="document-tools/index"></span><section id="tools">
<h2>Tools<a class="headerlink" href="#tools" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
</div>
</section>
</div>
</section>


              </article>
              
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, Facebook AI Research.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <!-- <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <p><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-what_is_xformers">What is xFormers?</a></li>
</ul>
<p><span class="caption-text">Components Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-components/index">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-components/attentions">Attention mechanisms</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-components/feedforward">Feedforward mechanisms</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-components/position_embedding">Position Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-components/reversible">Reversible layer</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-components/mha">Multi Head Attention</a></li>
</ul>
</li>
</ul>
<p><span class="caption-text">Factory</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-factory/index">Factory</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-factory/block">Block factory</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-factory/model">Model factory</a></li>
</ul>
</li>
</ul>
<p><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-tutorials/index">Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-tutorials/sparse_vit">Replace all attentions from an existing ViT model with a sparse equivalent?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-tutorials/blocksparse">Using BlockSparseAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-tutorials/extend_attentions">Extend the xFormers parts zoo</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-tutorials/use_attention">I’m only interested in testing out the attention mechanisms that are hosted here</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-tutorials/pytorch_encoder">I’m used to PyTorch Transformer Encoder, do you have an equivalent?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-tutorials/reversible">Using the Reversible block</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-tutorials/triton">Using Triton-based layers</a></li>
</ul>
</li>
</ul>
<p><span class="caption-text">Custom parts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-custom_parts/index">Custom parts reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#sparse-cuda-kernels">Sparse CUDA kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#triton-parts">Triton parts</a></li>
</ul>
</li>
</ul>
<p><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-tools/index">Tools</a><ul class="simple">
</ul>
</li>
</ul>

            </div>
          </div>
        </div> -->
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="./"
    src="_static/documentation_options.js"></script>
  <script src="_static/jquery.js"></script>
  <script src="_static/underscore.js"></script>
  <script src="_static/doctools.js"></script>
  <script src="_static/language_data.js"></script>
  

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://github.com/facebookresearch/xformers" class="footer-logo"></a>
      </div>
      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://github.com/facebookresearch/xformers">fairscale</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="">Resources</a></li>
            <li><a href="https://github.com/facebookresearch/xformers">Docs</a></li>
            <li><a href="https://github.com/facebookresearch/xformers/issues" target="_blank">Github Issues</a></li>
          </ul>
        </div>
      </div>
    </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://github.com/facebookresearch/xformers" aria-label="fairscale"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/facebookresearch/xformers/blob/master/README.md">Get Started</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Docs</a>
          </li>

          <li>
            <a href="https://github.com/facebookresearch/xformers">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>